import requests
import os
import time
import pandas as pd
from bs4 import BeautifulSoup
from io import StringIO
from urllib.parse import urlparse, unquote

# í˜„ì¬ê°€
def get_current_price(name, code) :
    
    URL = f"https://finance.naver.com/item/sise.naver?code={code}"
    
    # 1. 'User-Agent' ì„¤ì • (ë„¤ì´ë²„ëŠ” ë´‡(Bot) ì ‘ê·¼ì„ ë§‰ì„ ìˆ˜ ìˆìŒ)
    #    - ë§ˆì¹˜ ì¼ë°˜ì ì¸ ì›¹ ë¸Œë¼ìš°ì €(í¬ë¡¬ ë“±)ì—ì„œ ì ‘ì†í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì†ì—¬ì£¼ëŠ” í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Referer' : f'https://finance.naver.com/item/sise.naver?code={code}'
    }
    
    with requests.Session() as s :
        try:
            s.headers.update(headers)

            # 2. requestsë¥¼ ì‚¬ìš©í•´ URLì— GET ìš”ì²­ (í—¤ë” í¬í•¨)
            #   - response.status_code : 200 ì„±ê³µ, 404 Not Found, 403 Forbidden, 500 Internal Server Error
            #   - response.ok : status_codeê°€ ì„±ê³µ ë²”ìœ„ë©´ True, ì˜¤ë¥˜ ë²”ìœ„ë©´ False
            #   - response.raise_for_status() : status_codeê°€ 4xx, 5xx ê°™ì€ ì˜¤ë¥˜ ì‹œ Exception ë°œìƒ
            #   - response.text : ì„œë²„ê°€ ë³´ë‚¸ ë””ì½”ë”©ëœ ë°ì´í„°
            #   - response.content : ì„œë²„ê°€ ë³´ë‚¸ raw bytes
            #   - response.json() : ì‘ë‹µ ë‚´ìš©ì´ jsonì¼ ê²½ìš° íŒŒì‹±í•˜ì—¬ dict or listë¡œ ë³€í™˜
            #   - response.headers : ì‘ë‹µ í—¤ë” ì •ë³´, ['Content-Type'], ['Set-Cookie'], ['Server']
            response = s.get(URL, headers=headers, timeout=10)
            response.raise_for_status() # 200 OK ìƒíƒœ ì½”ë“œê°€ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ
            time.sleep(1)
            
            # 3. BeautifulSoupì„ ì‚¬ìš©í•´ HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 4. ì›í•˜ëŠ” ì •ë³´(í˜„ì¬ê°€)ê°€ ìˆëŠ” HTML ìš”ì†Œë¥¼ ì„ íƒ (CSS Selector)
            #    - ë„¤ì´ë²„ ì¦ê¶Œ í˜ì´ì§€ì—ì„œ í˜„ì¬ê°€ëŠ” 'strong' íƒœê·¸ì™€ 'id'ê°€ '_nowVal'ì¸ ìš”ì†Œì— ìˆìŒ
            #    - ì´ ì„ íƒìëŠ” ë„¤ì´ë²„ê°€ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ë¥¼ ë°”ê¾¸ë©´ ë³€ê²½ë  ìˆ˜ ìˆìŒ
            price_element = soup.select_one('strong#_nowVal')

            # 5. ì „ì¼ëŒ€ë¹„ ë“±ë½ì•¡ ìš”ì†Œ ì„ íƒ (í´ë˜ìŠ¤ë‚˜ IDëŠ” ë„¤ì´ë²„ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            # ë„¤ì´ë²„ ì‹œì„¸ í˜ì´ì§€ì—ì„œ ì „ì¼ëŒ€ë¹„ ë“±ë½ì•¡ì€ ë³´í†µ '_diff'ë‚˜ '_dltp' IDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            diff_element = soup.select_one('strong#_diff') 
            
            # 6. ì „ì¼ëŒ€ë¹„ ë“±ë½ë¥  ìš”ì†Œ ì„ íƒ
            # ë“±ë½ë¥ ì€ ë³´í†µ '_rate' IDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            rate_element = soup.select_one('strong#_rate')
            
            if price_element:
                current_price = price_element.get_text().strip()
                diff_price = diff_element.get_text().strip()
                diff_price = "".join(diff_price.split())
                diff_rate = rate_element.get_text().strip()
                current_price_text = f"<{name}> í˜„ì¬ê°€: {current_price} {diff_price}({diff_rate})"
                print(f"âœ… {current_price_text}")
                return current_price_text
            else:
                print("âŒ í˜„ì¬ê°€ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                print("    (ë„¤ì´ë²„ ì¦ê¶Œ í˜ì´ì§€ì˜ HTML êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")
                return ""

        except requests.exceptions.HTTPError as e:
            print(f"âŒ HTTP ì˜¤ë¥˜ ë°œìƒ: {e}")
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}")
        except Exception as e:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""


# íˆ¬ììë³„ ë§¤ë§¤ë™í–¥
def get_investor_trading_volume(name, code, page) :

    URL = f"https://finance.naver.com/item/frgn.naver?code={code}&page={page}"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Referer' : f'https://finance.naver.com/item/sise.naver?code={code}'
    }

    trading_data = pd.DataFrame()
    with requests.Session() as s :
        try :
            s.headers.update(headers)
            s.get(f'https://finance.naver.com/item/sise.naver?code={code}', headers=headers, timeout=10)
            time.sleep(1)

            response = s.get(URL)
            response.raise_for_status()
            
            # 4. pandasì˜ read_htmlë¡œ í˜ì´ì§€ì˜ ëª¨ë“  <table> ì½ê¸°
            #    - read_htmlì€ HTML í…ìŠ¤íŠ¸ì—ì„œ <table> íƒœê·¸ë¥¼ ëª¨ë‘ ì°¾ì•„ DataFrameì˜ ë¦¬ìŠ¤íŠ¸(List)ë¡œ ë°˜í™˜
            tables = pd.read_html(StringIO(response.text))

            # 5. ì›í•˜ëŠ” í…Œì´ë¸” ì„ íƒ
            #    - ë„¤ì´ë²„ ì¦ê¶Œì˜ í•´ë‹¹ í˜ì´ì§€ì—ëŠ” ì—¬ëŸ¬ í…Œì´ë¸”ì´ ìˆì–´ ì›í•˜ëŠ” ë°ì´í„° index ì°¾ì•„ì•¼ í•¨
            #    - NXT íƒ­ì´ ì—†ëŠ” ì¢…ëª©ì€ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìš”
            if len(tables) > 0:
                if name == "ì”¨ì–´ìŠ¤í…Œí¬ë†€ë¡œì§€" or name == "ì¹´ì¹´ì˜¤" or name == "í•œêµ­ì „ë ¥":
                    trading_data = tables[2]
                else :
                    trading_data = tables[3]
                #print(trading_data)

                # 6. MultiIndexë¡œ ëœ ì»¬ëŸ¼ ì •ë¦¬
                #trading_data.columns = ['_'.join(col).strip() for col in trading_data.columns.values] # MultiIndex ì •ë¦¬
                trading_data.columns = ['ë‚ ì§œ', 'ì¢…ê°€', 'ì „ì¼ë¹„', ' ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰', 'ê¸°ê´€_ìˆœë§¤ë§¤ëŸ‰', 'ì™¸êµ­ì¸_ìˆœë§¤ë§¤ëŸ‰', 'ì™¸êµ­ì¸_ë³´ìœ ì£¼ìˆ˜', 'ì™¸êµ­ì¸_ë³´ìœ ìœ¨']
                trading_data = trading_data.dropna(subset=['ë‚ ì§œ'])
                
                print(f"âœ… <{name}> ë§¤ë§¤ë™í–¥ {page}")
                print(trading_data.head())
                
            else:
                print("âŒ í˜ì´ì§€ì—ì„œ í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        except requests.exceptions.HTTPError as e:
            print(f"âŒ HTTP ì˜¤ë¥˜ ë°œìƒ: {e}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return trading_data


# ì¢…ëª© ë¶„ì„ ë¦¬í¬íŠ¸ ëª©ë¡
def get_research_reports(name, code) :

    URL = f"https://finance.naver.com/research/company_list.naver?keyword=&brokerCode=&writeFromDate=&writeToDate=&searchType=itemCode&itemName=%BB%EF%BC%BA%C0%FC%C0%DA&itemCode={code}&x=38&y=18&page=1"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Referer' : 'https://finance.naver.com/research/'
    }

    BASE_URL = "https://finance.naver.com/research/"
    reports_list = []

    with requests.Session() as s :
        try :
            s.headers.update(headers)
            response = s.get(URL, headers=headers, timeout=10)
            response.raise_for_status()
            time.sleep(1)

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë¦¬í¬íŠ¸ ëª©ë¡ì€ <table class="type_1"> ì•ˆì˜ <tr>(í–‰)ì— ìˆìŠµë‹ˆë‹¤.
            # 'class'ê°€ ì—†ëŠ” <tr> íƒœê·¸ë§Œ ì„ íƒ (í—¤ë”/êµ¬ë¶„ì„  ì œì™¸)
            report_rows = soup.select('table.type_1 tr:not([class])')

            if not report_rows:
                print(f"ë¦¬í¬íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ë§ˆì§€ë§‰ í˜ì´ì§€)")
                return None
            
            for row in report_rows:
                # ê° í–‰(tr)ì—ì„œ td(ì¹¸)ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.
                cells = row.find_all('td')
                #print(cells)
                
                # [ì œëª©, ì¦ê¶Œì‚¬, (íŒŒì¼), ë‚ ì§œ] êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
                if len(cells) < 3: # ìµœì†Œ 3ì¹¸(ì œëª©, ì¦ê¶Œì‚¬, ë‚ ì§œ)ì´ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
                    continue
                    
                # 1. ì œëª© ë° ë§í¬ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì¹¸)
                link_tag = cells[1].find('a')
                #print(link_tag)
                if not link_tag:
                    continue
                    
                title = link_tag.get_text(strip=True)
                relative_link = link_tag['href']
                full_link = BASE_URL + relative_link # '/research/...' -> 'https://...'
                
                # 2. ì¦ê¶Œì‚¬ ì¶”ì¶œ (ë‘ ë²ˆì§¸ ì¹¸)
                source = cells[2].get_text(strip=True)
                
                # 3. ë‚ ì§œ ì¶”ì¶œ (ë§ˆì§€ë§‰ ì¹¸)
                date = cells[4].get_text(strip=True)
                
                #print(title, relative_link, full_link, source, date)
                
                # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
                reports_list.append({
                    'title': title,
                    'source': source,
                    'date': date,
                    'link': full_link
                })

            print(f"âœ… <{name}> ë¦¬í¬íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ")
            
        except requests.exceptions.HTTPError as e:
            print(f"âŒ HTTP ì˜¤ë¥˜ ë°œìƒ: {e}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return reports_list


# ì¢…ëª© ë¦¬í¬íŠ¸ í˜ì´ì§€ì—ì„œ PDF ë§í¬ URL ì¶”ì¶œ
def extract_pdf_download_url(report_detail_url):
   
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Referer' : 'https://finance.naver.com/research/'
    }
    
    with requests.Session() as s :
        try:
            s.headers.update(headers) # ì„¸ì…˜ì— ê¸°ë³¸ í—¤ë” ì—…ë°ì´íŠ¸
            response = s.get(report_detail_url, headers=headers, timeout=10)
            response.raise_for_status()
            time.sleep(1)
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. PDF ë§í¬ê°€ ìœ„ì¹˜í•  ê°€ëŠ¥ì„±ì´ ë†’ì€ íŠ¹ì • ì˜ì—­ì„ ê²€ìƒ‰ (ì˜ˆ: table.view tbody)
            # ë„¤ì´ë²„ ì¦ê¶Œì˜ ë¦¬í¬íŠ¸ ìƒì„¸ í˜ì´ì§€ëŠ” ë³´í†µ ì²¨ë¶€íŒŒì¼ ë§í¬ë¥¼ <a> íƒœê·¸ë¡œ ì œê³µí•©ë‹ˆë‹¤.
            
            # 2. 'pdf' ë˜ëŠ” 'ë‹¤ìš´ë¡œë“œ' ë¬¸êµ¬ê°€ í¬í•¨ëœ ë§í¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            # ê°€ì¥ í”í•œ êµ¬ì¡°: <a> íƒœê·¸ì˜ hrefê°€ .pdfë¡œ ëë‚˜ëŠ” ê²½ìš°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            pdf_link_tag = soup.find('a', href=lambda href: href and ('.pdf' in href.lower()))
            
            if pdf_link_tag:
                relative_link = pdf_link_tag['href']
                #print(relative_link)
                
                # ë§í¬ê°€ ìƒëŒ€ ê²½ë¡œì¼ ê²½ìš° (ì˜ˆ: /imgstock/upload/...)
                if relative_link.startswith('/'):
                    BASE_URL = "https://finance.naver.com"
                    full_pdf_url = BASE_URL + relative_link
                else:
                    full_pdf_url = relative_link
                    
                return full_pdf_url
            
            return None # PDF ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨

        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ ({report_detail_url}): {e}")
            return None
        except Exception as e:
            print(f"âŒ ê¸°íƒ€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None


# ì¢…ëª© ë¶„ì„ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ
def download_pdf_report(name, source, pdf_url, save_directory):
      
    # 1. User-Agent ì„¤ì • (ì„œë²„ ì°¨ë‹¨ ë°©ì§€)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Referer' : 'https://finance.naver.com/research/company_list.naver'
    }
    
    # 2. íŒŒì¼ëª… ê²°ì •
    try:
        # URLì—ì„œ íŒŒì¼ëª…ì„ ì¶”ì¶œ (ê²½ë¡œì˜ ë§ˆì§€ë§‰ ë¶€ë¶„)
        parsed_url = urlparse(pdf_url)
        # URL ë””ì½”ë”© (í•œê¸€ íŒŒì¼ëª… ì²˜ë¦¬) í›„, ê²½ë¡œì˜ ë§ˆì§€ë§‰ ë¶€ë¶„(íŒŒì¼ëª…)ë§Œ ì¶”ì¶œ
        file_name = f"{name}_{source}_{unquote(os.path.basename(parsed_url.path))}"
        if not file_name.endswith('.pdf'):
            # íŒŒì¼ í™•ì¥ìê°€ .pdfê°€ ì•„ë‹ˆê±°ë‚˜ ì´ìƒí•˜ë©´ ê¸°ë³¸ íŒŒì¼ëª… ì§€ì •
            file_name = file_name + ".pdf" 

    except Exception:
        file_name = f"{name}_{source}_default_report.pdf"

    # 3. ì €ì¥ ê²½ë¡œ ìƒì„±
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)
        print(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±: {save_directory}")
        
    save_path = os.path.join(save_directory, file_name)
        
    with requests.Session() as s :
        # 4. íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìš”ì²­
        try:
            s.headers.update(headers) # ì„¸ì…˜ì— ê¸°ë³¸ í—¤ë” ì—…ë°ì´íŠ¸
            response = s.get(pdf_url, headers=headers, stream=True)
            response.raise_for_status() # HTTP ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ
            time.sleep(1)

            # 5. Content-Type í—¤ë” ê²€ì¦ (ì‘ë‹µì´ PDFì¸ì§€ í™•ì¸)
            content_type = response.headers.get('Content-Type', '').lower()
            if 'application/pdf' not in content_type and 'application/octet-stream' not in content_type:
                # Content-Typeì´ 'text/html'ì´ë©´ì„œ PDF ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°ë¥¼ í¬ì°©
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: Content-Typeì´ PDF ê´€ë ¨ ìœ í˜•ì´ ì•„ë‹™ë‹ˆë‹¤. Content-Type: {content_type}")
                print(f"   ì‘ë‹µ ì‹œì‘ ë¶€ë¶„ (ë””ë²„ê¹…): {response.text[:100].strip()}...") 
                return None
            
            # 6. ë°”ì´ë„ˆë¦¬ íŒŒì¼ë¡œ ì €ì¥ (wb: write binary)
            with open(save_path, 'wb') as f:
                # response.iter_content(chunk_size=8192) : ë°ì´í„°ë¥¼ 8KBì”© ëŠì–´ì„œ ì €ì¥
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk: # í•„í„°ë§ (ê°„í˜¹ keep-alive ì²­í¬ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
                        f.write(chunk)
            
            # 7. ì €ì¥ í›„ íŒŒì¼ í¬ê¸° ê²€ì¦ (0KB íŒŒì¼ ë°©ì§€)
            if os.path.getsize(save_path) == 0:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: íŒŒì¼ í¬ê¸°ê°€ 0KBì…ë‹ˆë‹¤. íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.")
                os.remove(save_path)
                return None
            
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: '{file_name}' ì €ì¥ ì™„ë£Œ")
            return save_path
            
        except requests.exceptions.HTTPError as e:
            print(f"âŒ HTTP ì˜¤ë¥˜ ë°œìƒ (URL: {pdf_url}): ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜ {e}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ (URL: {pdf_url}): {e}")
            return None
        except Exception as e:
            print(f"âŒ ê¸°íƒ€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None


if __name__ == "__main__" :  
    get_current_price("ì‚¼ì„±ì „ì", "005930")
    exit()
    final_data = pd.DataFrame()
    for i in range(1, 3) :
        data = get_investor_trading_volume("ì‚¼ì„±ì „ì", "005930", i)
        final_data = pd.concat([final_data, data], ignore_index=True)
        time.sleep(2)
    print(final_data.loc[0:2, 'ì™¸êµ­ì¸_ìˆœë§¤ë§¤ëŸ‰'].sum())

    reports_list = get_research_reports("ì‚¼ì„±ì „ì", "005930")
    print(reports_list[1]['link'])

    pdf_url = extract_pdf_download_url(reports_list[1]['link'])
    
    download_pdf_report("ì‚¼ì„±ì „ì", 1, pdf_url)
    